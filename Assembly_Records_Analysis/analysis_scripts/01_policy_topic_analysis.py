#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
êµ­íšŒ ì •ì±… í† í”½ ë³€í™” ì¶”ì´ ë¶„ì„ - ëª©í‘œì— ì§‘ì¤‘í•œ ìµœê³ ì˜ ë¶„ì„
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter, defaultdict
import re
from datetime import datetime
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

def load_policy_data():
    """ì •ì±… ê´€ë ¨ ë°ì´í„°ë§Œ ë¡œë“œ"""
    print("ğŸ“Š ì •ì±… ê´€ë ¨ ë°ì´í„° ë¡œë”© ì¤‘...")
    
    all_data = []
    data_dir = 'data/with_party'
    sessions = [d for d in os.listdir(data_dir) if d.startswith('ì œ') and os.path.isdir(os.path.join(data_dir, d))]
    sessions.sort()
    
    for session in sessions:
        session_dir = os.path.join(data_dir, session)
        speech_files = [f for f in os.listdir(session_dir) if 'speeches' in f]
        
        for file in speech_files:
            file_path = os.path.join(session_dir, file)
            df = pd.read_csv(file_path)
            df['session_name'] = session
            df['file_name'] = file
            all_data.append(df)
    
    combined_df = pd.concat(all_data, ignore_index=True)
    print(f"âœ… ì´ {len(combined_df):,}ê°œ ë°œì–¸ ë¡œë“œ ì™„ë£Œ")
    return combined_df

def filter_policy_speeches(df):
    """ì •ì±… ê´€ë ¨ ë°œì–¸ë§Œ í•„í„°ë§"""
    print("\n ì •ì±… ê´€ë ¨ ë°œì–¸ í•„í„°ë§ ì¤‘...")
    
    # ì •ì±… ê´€ë ¨ í‚¤ì›Œë“œ
    policy_keywords = [
        'ì •ì±…', 'ë²•ì•ˆ', 'ì˜ˆì‚°', 'ë³µì§€', 'ê²½ì œ', 'ì•ˆì „', 'ì§€ë°©', 'êµ­ë¯¼', 'ì‚¬íšŒ', 'ë°œì „',
        'í•´ê²°', 'ë¬¸ì œ', 'ê³¼ì œ', 'ì§€ì›', 'ëŒ€ì±…', 'ë°©ì•ˆ', 'ì œë„', 'ê°œì„ ', 'ê°•í™”', 'ì¶”ì§„',
        'ê³„íš', 'ë°©í–¥', 'ëª©í‘œ', 'í˜„ì•ˆ', 'ì§ˆì˜', 'ê°ì‚¬', 'ì²­ë¬¸íšŒ', 'ì¸ì‚¬', 'ì„ëª…',
        'ì¬ë‚œ', 'ì†Œë°©', 'ê²½ì°°', 'ë¶„ê¶Œ', 'ì§€ì—­', 'ì„ ê±°', 'íˆ¬í‘œ', 'ê¸°ê¸ˆ', 'ê²°ì‚°'
    ]
    
    # ì˜ì‚¬ì§„í–‰ í‚¤ì›Œë“œ (ì œì™¸í•  í‚¤ì›Œë“œ)
    procedural_keywords = [
        'ì˜ì„ì„ ì •ëˆí•´', 'ì„±ì›ì´ ë˜ì—ˆìœ¼ë¯€ë¡œ', 'íšŒì˜ë¥¼ ê°œìµœ', 'íšŒì˜ë¥¼ ê°œíšŒ',
        'ê°ì‚¬ì˜ ë§ì”€', 'ê³ ë§™ìŠµë‹ˆë‹¤', 'ë‹¤ìŒì€', 'ë§ì”€í•´ ì£¼ì‹­ì‹œì˜¤', 'ì´ì˜ ì—†ìœ¼ì‹­ë‹ˆê¹Œ',
        'ì˜ê²°í•˜ê² ìŠµë‹ˆë‹¤', 'ìƒì •í•©ë‹ˆë‹¤', 'ì•ˆê±´ì„ ìƒì •', 'íšŒì˜ë¥¼ ë§ˆì¹˜ê² ìŠµë‹ˆë‹¤',
        'ì¢‹ìŠµë‹ˆë‹¤', 'ë„¤', 'ì˜ˆ', 'ì•„ë‹ˆìš”', 'ê·¸ë ‡ìŠµë‹ˆë‹¤', 'ë§ìŠµë‹ˆë‹¤'
    ]
    
    def is_policy_speech(text, speaker_position):
        if pd.isna(text):
            return False
        
        text_str = str(text).lower()
        speaker_pos = str(speaker_position).lower()
        
        # ìœ„ì›ì¥ì˜ ì˜ì‚¬ì§„í–‰ ë°œì–¸ ì œì™¸
        if 'ìœ„ì›ì¥' in speaker_pos:
            if any(keyword in text_str for keyword in procedural_keywords):
                return False
        
        # ì •ì±… ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€
        policy_count = sum(1 for keyword in policy_keywords if keyword in text_str)
        
        # ë°œì–¸ ê¸¸ì´ ê³ ë ¤ (ë„ˆë¬´ ì§§ìœ¼ë©´ ì œì™¸)
        if len(text_str) < 50:
            return False
        
        return policy_count >= 2  # 2ê°œ ì´ìƒì˜ ì •ì±… í‚¤ì›Œë“œ í¬í•¨
    
    # ì •ì±… ë°œì–¸ í•„í„°ë§
    policy_mask = df.apply(lambda row: is_policy_speech(row['speech_text'], row['speaker_position']), axis=1)
    policy_df = df[policy_mask].copy()
    
    print(f"ğŸ“Š ì •ì±… ë°œì–¸ í•„í„°ë§ ê²°ê³¼:")
    print(f"  - ì „ì²´ ë°œì–¸: {len(df):,}ê°œ")
    print(f"  - ì •ì±… ë°œì–¸: {len(policy_df):,}ê°œ ({len(policy_df)/len(df)*100:.1f}%)")
    
    return policy_df

def extract_policy_topics(df):
    """ì •ì±… í† í”½ ì¶”ì¶œ"""
    print("\n ì •ì±… í† í”½ ì¶”ì¶œ ì¤‘...")
    
    # ì •ì±… ì˜ì—­ë³„ í‚¤ì›Œë“œ ì •ì˜
    policy_areas = {
        'ì•ˆì „ê´€ë¦¬': {
            'keywords': ['ì•ˆì „', 'ì¬ë‚œ', 'ì†Œë°©', 'ê²½ì°°', 'ì‚¬ê³ ', 'ì°¸ì‚¬', 'í™”ì¬', 'ì•ˆì „ê´€ë¦¬', 'ì¬ë‚œì•ˆì „',
                        'ì†Œë°©ì²­', 'ê²½ì°°ì²­', 'ì•ˆì „ì‚¬ê³ ', 'ì¬í•´', 'ë°©ì¬', 'êµ¬ì¡°', 'ì‘ê¸‰', '119', '112',
                        'í–‰ì •ì•ˆì „ë¶€', 'ì¬ë‚œê´€ë¦¬', 'ì•ˆì „ì •ì±…', 'ì¬ë‚œëŒ€ì‘', 'ì†Œë°©ì •ì±…', 'ê²½ì°°ì •ì±…'],
            'weight': 1.0
        },
        'ì§€ë°©ë¶„ê¶Œ': {
            'keywords': ['ì§€ë°©', 'ë¶„ê¶Œ', 'ì§€ì—­', 'ì§€ìì²´', 'ì§€ë°©ìì¹˜', 'ê· í˜•ë°œì „', 'ì§€ë°©ì„¸', 'ì§€ì—­ê· í˜•',
                        'ì§€ë°©ë¶„ê¶Œ', 'ì§€ì—­ë°œì „', 'ì§€ë°©ì •ë¶€', 'ì§€ì—­ê²½ì œ', 'ì§€ë°©ê³µê¸°ì—…', 'ì§€ë°©êµë¶€ì„¸',
                        'ì¸êµ¬ê°ì†Œì§€ì—­', 'ì ‘ê²½ì§€ì—­', 'ì§€ë°©ìì¹˜ë²•', 'ì§€ì—­ê· í˜•ë°œì „'],
            'weight': 1.0
        },
        'ì˜ˆì‚°ê²°ì‚°': {
            'keywords': ['ì˜ˆì‚°', 'ê²°ì‚°', 'íšŒê³„', 'ê¸°ê¸ˆ', 'ì¬ì •', 'ì˜ˆì‚°ì•ˆ', 'ê²°ì‚°ì„œ', 'íšŒê³„ì—°ë„',
                        'ì˜ˆì‚°ì§‘í–‰', 'ì¬ì •ì§€ì›', 'ë³´ì¡°ê¸ˆ', 'ì§€ë°©êµë¶€ì„¸', 'ê¸°ê¸ˆê´€ë¦¬', 'ì¬ì •ì •ì±…',
                        'ì˜ˆì‚°í¸ì„±', 'ì˜ˆì‚°ì‹¬ì˜', 'ì˜ˆì‚°ì§‘í–‰'],
            'weight': 1.0
        },
        'ì¸ì‚¬ê´€ë¦¬': {
            'keywords': ['ì¸ì‚¬', 'ì„ëª…', 'ì²­ë¬¸íšŒ', 'í›„ë³´ì', 'ì¥ê´€', 'ìœ„ì›ì¥', 'ì¸ì‚¬í˜ì‹ ', 'ê³µë¬´ì›',
                        'ì„ìš©', 'ìŠ¹ì§„', 'ì¸ì‚¬ì œë„', 'ì¸ì‚¬ì •ì±…', 'ì¸ì‚¬í˜ì‹ ì²˜', 'ì¸ì‚¬ì²­ë¬¸íšŒ',
                        'ì¸ì‚¬ì²­ë¬¸ìš”ì²­ì•ˆ', 'ì¸ì‚¬í˜ì‹ ì²˜ì¥'],
            'weight': 1.0
        },
        'ë²•ì•ˆì‹¬ì‚¬': {
            'keywords': ['ë²•ì•ˆ', 'ë²•ë¥ ', 'ê°œì •', 'ì œì •', 'ì‹¬ì‚¬', 'í†µê³¼', 'ë²•ë¥ ì•ˆ', 'ë²•ì•ˆì‹¬ì‚¬',
                        'ì…ë²•', 'ë²•ì œ', 'ë²•ë ¹', 'ë²•ê·œ', 'íŠ¹ë³„ì¡°ì¹˜ë²•ì•ˆ', 'íŠ¹ë³„ë²•ì•ˆ', 'ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ'],
            'weight': 1.0
        },
        'êµ­ì •ê°ì‚¬': {
            'keywords': ['ê°ì‚¬', 'í˜„ì•ˆ', 'ì§ˆì˜', 'ì¡°ì‚¬', 'ì²­ë¬¸íšŒ', 'êµ­ì •ì¡°ì‚¬', 'ê°ì‚¬ì›', 'ê°ì‚¬ìœ„ì›',
                        'í˜„ì•ˆì§ˆì˜', 'êµ­ì •ê°ì‚¬', 'í”¼í•´ì¡°ì‚¬', 'í˜„ì•ˆì§ˆì˜'],
            'weight': 1.0
        },
        'ì„ ê±°ê´€ë¦¬': {
            'keywords': ['ì„ ê±°', 'íˆ¬í‘œ', 'ì„ ê±°ê´€ë¦¬', 'ì„ ê´€ìœ„', 'ì„ ê±°ì œë„', 'ì„ ê±°ë²•', 'ê³µì§ì„ ê±°',
                        'ì„ ê±°ê³µì˜ì œ', 'ì„ ê±°ë¹„ìš©', 'ì¤‘ì•™ì„ ê±°ê´€ë¦¬ìœ„ì›íšŒ', 'êµ­ë¯¼íˆ¬í‘œë²•', 'ì§€ë°©ì„ ê±°'],
            'weight': 1.0
        },
        'ë³µì§€ì •ì±…': {
            'keywords': ['ë³µì§€', 'ë³µì§€ì •ì±…', 'ë³µì§€ì œë„', 'ë³µì§€í˜œíƒ', 'ë³µì§€ì„œë¹„ìŠ¤', 'ì‚¬íšŒë³´ì¥',
                        'ë³µì§€êµ­ê°€', 'ë³µì§€ì˜ˆì‚°', 'ë¯¼ìƒíšŒë³µì§€ì›ê¸ˆ', 'ë¯¼ìƒíšŒë³µì§€ì›ë²•', 'ë¯¼ìƒ'],
            'weight': 1.0
        },
        'ê²½ì œì •ì±…': {
            'keywords': ['ê²½ì œ', 'ê²½ì œì •ì±…', 'ê²½ì œì„±ì¥', 'ê²½ì œë°œì „', 'ê²½ì œì§€í‘œ', 'ê²½ì œì§€ì›',
                        'ê²½ì œí™œì„±í™”', 'ê²½ì œíšŒë³µ', 'ê²½ì œí™œë™', 'ê²½ì œì ', 'ê²½ì œììœ íŠ¹êµ¬'],
            'weight': 1.0
        }
    }
    
    # ê° ë°œì–¸ì„ ì •ì±… ì˜ì—­ë³„ë¡œ ë¶„ë¥˜
    def classify_policy_area(text):
        if pd.isna(text):
            return 'ê¸°íƒ€'
        
        text_str = str(text).lower()
        area_scores = {}
        
        for area, info in policy_areas.items():
            score = 0
            for keyword in info['keywords']:
                if keyword in text_str:
                    score += info['weight']
            area_scores[area] = score
        
        if max(area_scores.values()) > 0:
            return max(area_scores, key=area_scores.get)
        else:
            return 'ê¸°íƒ€'
    
    df['policy_area'] = df['speech_text'].apply(classify_policy_area)
    
    # ì •ì±… ì˜ì—­ë³„ í†µê³„
    area_counts = df['policy_area'].value_counts()
    print(f"ğŸ“Š ì •ì±… ì˜ì—­ë³„ ë°œì–¸ ìˆ˜:")
    for area, count in area_counts.items():
        print(f"  - {area}: {count:,}ê°œ ({count/len(df)*100:.1f}%)")
    
    return df, policy_areas

def analyze_temporal_trends(df):
    """ì˜ì—­ë³„ ë°œì–¸ ë³€í™” ì¶”ì´ ë¶„ì„"""
    print("\nì˜ì—­ë³„ ë°œì–¸ ë³€í™” ì¶”ì´ ë¶„ì„ ì¤‘...")
    
    # íšŒê¸°ë³„ ì •ì±… ì˜ì—­ë³„ ë°œì–¸ ìˆ˜
    trend_data = df.groupby(['session_name', 'policy_area']).size().unstack(fill_value=0)
    
    # íšŒê¸° ë²ˆí˜¸ ì¶”ì¶œ ë° ì •ë ¬
    def extract_session_number(session_name):
        return int(session_name.replace('ì œ', '').replace('íšŒ', ''))
    
    trend_data['session_num'] = trend_data.index.map(extract_session_number)
    trend_data = trend_data.sort_values('session_num')
    
    # ì •ì±… ì˜ì—­ë³„ ì´ ë°œì–¸ ìˆ˜
    area_totals = trend_data.drop('session_num', axis=1).sum().sort_values(ascending=False)
    
    print(f"ğŸ“Š ì •ì±… ì˜ì—­ë³„ ì´ ë°œì–¸ ìˆ˜:")
    for area, count in area_totals.items():
        print(f"  - {area}: {count:,}ê°œ")
    
    return trend_data, area_totals

def analyze_policy_keywords(df):
    """ì •ì±… í‚¤ì›Œë“œ ë¶„ì„"""
    print("\nì •ì±… í‚¤ì›Œë“œ ë¶„ì„ ì¤‘...")
    
    # ì •ì±… ì˜ì—­ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ
    area_keywords = defaultdict(list)
    
    for area in df['policy_area'].unique():
        if area == 'ê¸°íƒ€':
            continue
        
        area_speeches = df[df['policy_area'] == area]['speech_text']
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ë²•)
        all_text = ' '.join(area_speeches.astype(str))
        words = re.findall(r'[ê°€-í£]{2,}', all_text)
        
        # ë¹ˆë„ ê³„ì‚°
        word_counts = Counter(words)
        
        # ìƒìœ„ í‚¤ì›Œë“œ ì €ì¥
        top_keywords = [word for word, count in word_counts.most_common(20)]
        area_keywords[area] = top_keywords
    
    print(f"ğŸ“Š ì •ì±… ì˜ì—­ë³„ ìƒìœ„ í‚¤ì›Œë“œ:")
    for area, keywords in area_keywords.items():
        print(f"  - {area}: {keywords[:10]}")
    
    return area_keywords

def create_comprehensive_visualizations(trend_data, area_totals, area_keywords):
    """ì¢…í•© ì‹œê°í™” ìƒì„±"""
    print("\nğŸ“Š ì¢…í•© ì‹œê°í™” ìƒì„± ì¤‘...")
    
    # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('analysis_results', exist_ok=True)
    
    # 1. ì •ì±… ì˜ì—­ë³„ ë°œì–¸ ìˆ˜ íˆíŠ¸ë§µ
    plt.figure(figsize=(15, 10))
    
    # ìƒìœ„ 8ê°œ ì •ì±… ì˜ì—­ë§Œ ì„ íƒ
    top_areas = area_totals.head(8).index
    heatmap_data = trend_data[top_areas]
    
    sns.heatmap(heatmap_data.T, 
                annot=True, 
                fmt='.0f', 
                cmap='YlOrRd',
                cbar_kws={'label': 'ë°œì–¸ ìˆ˜'})
    
    plt.title('íšŒê¸°ë³„ ì •ì±… ì˜ì—­ ë°œì–¸ ìˆ˜ íˆíŠ¸ë§µ', fontsize=16, fontweight='bold')
    plt.xlabel('íšŒê¸°', fontsize=12)
    plt.ylabel('ì •ì±… ì˜ì—­', fontsize=12)
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)
    
    plt.tight_layout()
    plt.savefig('analysis_results/01_policy_heatmap.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # 2. ì •ì±… ì˜ì—­ë³„ ë³€í™” ì¶”ì´
    plt.figure(figsize=(15, 8))
    
    # ìƒìœ„ 6ê°œ ì •ì±… ì˜ì—­ì˜ ë³€í™” ì¶”ì´
    top_6_areas = area_totals.head(6).index
    
    for area in top_6_areas:
        plt.plot(trend_data['session_num'], trend_data[area], 
                marker='o', linewidth=2, label=area)
    
    plt.title('íšŒê¸°ë³„ ì •ì±… ì˜ì—­ ë³€í™” ì¶”ì´', fontsize=16, fontweight='bold')
    plt.xlabel('íšŒê¸°', fontsize=12)
    plt.ylabel('ë°œì–¸ ìˆ˜', fontsize=12)
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('analysis_results/01_policy_trends.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # 3. ì •ì±… ì˜ì—­ë³„ ì´ ë°œì–¸ ìˆ˜
    plt.figure(figsize=(12, 8))
    
    plt.barh(range(len(area_totals)), area_totals.values)
    plt.yticks(range(len(area_totals)), area_totals.index)
    plt.title('ì •ì±… ì˜ì—­ë³„ ì´ ë°œì–¸ ìˆ˜', fontsize=16, fontweight='bold')
    plt.xlabel('ì´ ë°œì–¸ ìˆ˜', fontsize=12)
    plt.ylabel('ì •ì±… ì˜ì—­', fontsize=12)
    
    plt.tight_layout()
    plt.savefig('analysis_results/01_policy_totals.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    
    print("ğŸ“Š ì¢…í•© ì‹œê°í™” ì™„ë£Œ: analysis_results/01_policy_*.png")

def generate_insights(trend_data, area_totals, area_keywords):
    """ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
    print("\nğŸ’¡ ì •ì±… í† í”½ ë³€í™” ì¸ì‚¬ì´íŠ¸ ìƒì„± ì¤‘...")
    
    # 1. ê°€ì¥ ê´€ì‹¬ì´ ë†’ì€ ì •ì±… ì˜ì—­
    top_area = area_totals.index[0]
    top_count = area_totals.iloc[0]
    
    print(f"ğŸ† ê°€ì¥ ê´€ì‹¬ì´ ë†’ì€ ì •ì±… ì˜ì—­: {top_area} ({top_count:,}ê°œ ë°œì–¸)")
    
    # 2. íšŒê¸°ë³„ ë³€í™” íŒ¨í„´
    print(f"\nğŸ“ˆ íšŒê¸°ë³„ ë³€í™” íŒ¨í„´:")
    
    # ê° ì •ì±… ì˜ì—­ë³„ ìµœê³ ì  íšŒê¸° ì°¾ê¸°
    for area in area_totals.head(5).index:
        max_session = trend_data[area].idxmax()
        max_count = trend_data[area].max()
        print(f"  - {area}: {max_session}ì—ì„œ ìµœê³ ì  ({max_count}ê°œ ë°œì–¸)")
    
    # 3. ì •ì±… ìš°ì„ ìˆœìœ„ ë³€í™”
    print(f"\nğŸ¯ ì •ì±… ìš°ì„ ìˆœìœ„ ë³€í™”:")
    
    # ì´ˆê¸° íšŒê¸° vs ìµœê·¼ íšŒê¸° ë¹„êµ
    available_sessions = trend_data.index.tolist()
    early_sessions = [s for s in available_sessions if 'ì œ415íšŒ' in s or 'ì œ416íšŒ' in s]
    recent_sessions = [s for s in available_sessions if 'ì œ427íšŒ' in s or 'ì œ428íšŒ' in s or 'ì œ429íšŒ' in s]
    
    if early_sessions:
        early_totals = trend_data.loc[early_sessions].sum()
    else:
        early_totals = pd.Series()
    
    if recent_sessions:
        recent_totals = trend_data.loc[recent_sessions].sum()
    else:
        recent_totals = pd.Series()
    
    if len(early_totals) > 0:
        print(f"  ì´ˆê¸° íšŒê¸° ìƒìœ„ 3ê°œ:")
        for area in early_totals.nlargest(3).index:
            print(f"    - {area}: {early_totals[area]:,}ê°œ")
    else:
        print(f"  ì´ˆê¸° íšŒê¸° ë°ì´í„° ì—†ìŒ")
    
    if len(recent_totals) > 0:
        print(f"  ìµœê·¼ íšŒê¸° ìƒìœ„ 3ê°œ:")
        for area in recent_totals.nlargest(3).index:
            print(f"    - {area}: {recent_totals[area]:,}ê°œ")
    else:
        print(f"  ìµœê·¼ íšŒê¸° ë°ì´í„° ì—†ìŒ")
    
    # 4. ì •ì±… ì˜ì—­ë³„ í•µì‹¬ í‚¤ì›Œë“œ
    print(f"\nğŸ”‘ ì •ì±… ì˜ì—­ë³„ í•µì‹¬ í‚¤ì›Œë“œ:")
    for area in area_totals.head(5).index:
        if area in area_keywords:
            keywords = area_keywords[area][:5]
            print(f"  - {area}: {', '.join(keywords)}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("êµ­íšŒ ì •ì±… í† í”½ ë³€í™” ì¶”ì´ ë¶„ì„")
    print("="*80)
    
    # 1. ë°ì´í„° ë¡œë“œ
    df = load_policy_data()
    
    # 2. ì •ì±… ë°œì–¸ í•„í„°ë§
    policy_df = filter_policy_speeches(df)
    
    # 3. ì •ì±… í† í”½ ì¶”ì¶œ
    policy_df, policy_areas = extract_policy_topics(policy_df)
    
    # 4. ì‹œê³„ì—´ ë³€í™” ì¶”ì´ ë¶„ì„
    trend_data, area_totals = analyze_temporal_trends(policy_df)
    
    # 5. ì •ì±… í‚¤ì›Œë“œ ë¶„ì„
    area_keywords = analyze_policy_keywords(policy_df)
    
    # 6. ì¢…í•© ì‹œê°í™” ìƒì„±
    create_comprehensive_visualizations(trend_data, area_totals, area_keywords)
    
    # 7. ì¸ì‚¬ì´íŠ¸ ìƒì„±
    generate_insights(trend_data, area_totals, area_keywords)
    
    print("\n" + "="*80)
    print("âœ… ì •ì±… í† í”½ ë³€í™” ì¶”ì´ ë¶„ì„ ì™„ë£Œ!")
    print("="*80)
    print("í•µì‹¬ ê²°ê³¼:")
    print("- êµ­íšŒì˜ ì •ì±… ê´€ì‹¬ì‚¬ íŒŒì•…")
    print("- ì •ì±… ì˜ì—­ë³„ ë³€í™” ì¶”ì´ ë¶„ì„")
    print("- íšŒê¸°ë³„ ì •ì±… ìš°ì„ ìˆœìœ„ ë³€í™”")
    print("- ì •ì±… ì˜ì—­ë³„ í•µì‹¬ í‚¤ì›Œë“œ")
    
    return {
        'trend_data': trend_data,
        'area_totals': area_totals,
        'area_keywords': area_keywords,
        'policy_df': policy_df
    }

if __name__ == "__main__":
    results = main()
